{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27a06089-19ed-4a3d-810b-04f9fbfc9f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from torch.nn.modules.module import Module\n",
    "from torch.nn.parameter import Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57b14105-986d-41dc-9e86-42e9a26af4e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 3, 768]), torch.Size([10, 3, 3]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Scaled Dot-Product Attention proposed in \"Attention Is All You Need\"\n",
    "    Compute the dot products of the query with all keys, divide each by sqrt(dim),\n",
    "    and apply a softmax function to obtain the weights on the values\n",
    "\n",
    "    Args: dim, mask\n",
    "        dim (int): dimention of attention\n",
    "        mask (torch.Tensor): tensor containing indices to be masked\n",
    "\n",
    "    Inputs: query, key, value, mask\n",
    "        - **query** (batch, q_len, d_model): tensor containing projection vector for decoder.\n",
    "        - **key** (batch, k_len, d_model): tensor containing projection vector for encoder.\n",
    "        - **value** (batch, v_len, d_model): tensor containing features of the encoded input sequence.\n",
    "        - **mask** (-): tensor containing indices to be masked\n",
    "\n",
    "    Returns: context, attn\n",
    "        - **context**: tensor containing the context vector from attention mechanism.\n",
    "        - **attn**: tensor containing the attention (alignment) from the encoder outputs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim: int):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "        self.sqrt_dim = np.sqrt(dim)\n",
    "\n",
    "    def forward(\n",
    "        self, query: Tensor, key: Tensor, value: Tensor, mask: Optional[Tensor] = None\n",
    "    ) -> Tuple[Tensor, Tensor]:\n",
    "        score = torch.bmm(query, key.transpose(1, 2)) / self.sqrt_dim\n",
    "\n",
    "        if mask is not None:\n",
    "            score.masked_fill_(mask.view(score.size()), -float(\"Inf\"))\n",
    "\n",
    "        attn = F.softmax(score, -1)\n",
    "        context = torch.bmm(attn, value)\n",
    "        return context, attn\n",
    "\n",
    "\n",
    "input = torch.randn(10, 3, 768)\n",
    "model = ScaledDotProductAttention(dim=768)\n",
    "context, attn = model(input, input, input)\n",
    "context.shape, attn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad9a55d1-6b66-4b3b-a3b6-cd8eaa650dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([80, 3, 96])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 3, 768]), torch.Size([80, 3, 3]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Attention proposed in \"Attention Is All You Need\"\n",
    "    Instead of performing a single attention function with d_model-dimensional keys, values, and queries,\n",
    "    project the queries, keys and values h times with different, learned linear projections to d_head dimensions.\n",
    "    These are concatenated and once again projected, resulting in the final values.\n",
    "    Multi-head attention allows the model to jointly attend to information from different representation\n",
    "    subspaces at different positions.\n",
    "\n",
    "    MultiHead(Q, K, V) = Concat(head_1, ..., head_h) 路 W_o\n",
    "        where head_i = Attention(Q 路 W_q, K 路 W_k, V 路 W_v)\n",
    "\n",
    "    Args:\n",
    "        d_model (int): The dimension of keys / values / quries (default: 512)\n",
    "        num_heads (int): The number of attention heads. (default: 8)\n",
    "\n",
    "    Inputs: query, key, value, mask\n",
    "        - **query** (batch, q_len, d_model): In transformer, three different ways:\n",
    "            Case 1: come from previoys decoder layer\n",
    "            Case 2: come from the input embedding\n",
    "            Case 3: come from the output embedding (masked)\n",
    "\n",
    "        - **key** (batch, k_len, d_model): In transformer, three different ways:\n",
    "            Case 1: come from the output of the encoder\n",
    "            Case 2: come from the input embeddings\n",
    "            Case 3: come from the output embedding (masked)\n",
    "\n",
    "        - **value** (batch, v_len, d_model): In transformer, three different ways:\n",
    "            Case 1: come from the output of the encoder\n",
    "            Case 2: come from the input embeddings\n",
    "            Case 3: come from the output embedding (masked)\n",
    "\n",
    "        - **mask** (-): tensor containing indices to be masked\n",
    "\n",
    "    Returns: output, attn\n",
    "        - **output** (batch, output_len, dimensions): tensor containing the attended output features.\n",
    "        - **attn** (batch * num_heads, v_len): tensor containing the attention (alignment) from the encoder outputs.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int = 512, num_heads: int = 8):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        assert d_model % num_heads == 0, \"d_model % num_heads should be zero.\"\n",
    "\n",
    "        self.d_head = int(d_model / num_heads)\n",
    "        self.num_heads = num_heads\n",
    "        self.scaled_dot_attn = ScaledDotProductAttention(self.d_head)\n",
    "        self.query_proj = nn.Linear(d_model, self.d_head * num_heads)\n",
    "        self.key_proj = nn.Linear(d_model, self.d_head * num_heads)\n",
    "        self.value_proj = nn.Linear(d_model, self.d_head * num_heads)\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            query: Tensor,\n",
    "            key: Tensor,\n",
    "            value: Tensor,\n",
    "            mask: Optional[Tensor] = None\n",
    "    ) -> Tuple[Tensor, Tensor]:\n",
    "        batch_size = value.size(0)\n",
    "\n",
    "        query = self.query_proj(query).view(batch_size, -1, self.num_heads, self.d_head)  # BxQ_LENxNxD\n",
    "        key = self.key_proj(key).view(batch_size, -1, self.num_heads, self.d_head)      # BxK_LENxNxD\n",
    "        value = self.value_proj(value).view(batch_size, -1, self.num_heads, self.d_head)  # BxV_LENxNxD\n",
    "\n",
    "        query = query.permute(2, 0, 1, 3).contiguous().view(batch_size * self.num_heads, -1, self.d_head)  # BNxQ_LENxD\n",
    "        key = key.permute(2, 0, 1, 3).contiguous().view(batch_size * self.num_heads, -1, self.d_head)      # BNxK_LENxD\n",
    "        value = value.permute(2, 0, 1, 3).contiguous().view(batch_size * self.num_heads, -1, self.d_head)  # BNxV_LENxD\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1).repeat(1, self.num_heads, 1, 1)  # BxNxQ_LENxK_LEN\n",
    "\n",
    "        context, attn = self.scaled_dot_attn(query, key, value, mask)\n",
    "\n",
    "        context = context.view(self.num_heads, batch_size, -1, self.d_head)\n",
    "        context = context.permute(1, 2, 0, 3).contiguous().view(batch_size, -1, self.num_heads * self.d_head)  # BxTxND\n",
    "\n",
    "        return context, attn\n",
    "\n",
    "\n",
    "input = torch.randn(10, 3, 768)\n",
    "model = MultiHeadAttention(d_model=768,num_heads=8)\n",
    "context, attn = model(input, input, input)\n",
    "context.shape, attn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045d4a03-b7ca-430b-8dcf-12d1c4a2159d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
