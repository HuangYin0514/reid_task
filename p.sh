The objective of dynamical system learning tasks is to forecast the future behavior of a system by leveraging observed data 1. Dynamical systems provide a mathematical framework to describe the evolution of natural phenomena across time and space, commonly represented using differential equations. Solving these equations allows predictions about the future state of dynamical systems. Understanding complex physical dynamics across various spatial and temporal scales poses a significant research challenge, garnering attention from numerous scholars and having practical implications 2-6.
Prediction in dynamical systems primarily relies on creating models derived from first principles. However, due to incomplete knowledge, these models based on physical laws often oversimplify or misrepresent the underlying structure of dynamical systems 7. As a result, they tend to exhibit high biases and modeling errors that cannot be rectified by optimizing a few parameters. Consequently, it becomes essential to employ dynamical system learning methods that can accurately identify valid dynamic models based on observed trajectories. These methods play a crucial role in the analysis, simulation, and control of dynamic systems 8-10.
Dynamical systems learning methods can be broadly classified into two categories: purely data-driven methods and physics-guided machine learning methods 11. The availability of experimental data has led to the increasing popularity of purely data-driven methods, which utilize forward or backward models to fit the training data without requiring extensive involvement in model design or the derivation of learning and inference processes 12-15. Among purely data-driven methods, neural networks have emerged as the dominant technology due to their exceptional ability to efficiently capture high-dimensional spatio-temporal dynamics from large datasets. One notable method in this category is the sparse identification of nonlinear dynamics (SINDy), which represents dynamic nonlinear differential equations as linear combinations of nonlinear candidate functions and approximates the model through sparse regression 16. However, SINDy's reliance on numerical differentiation renders it sensitive to data noise, particularly when higher-order derivatives are involved. To mitigate this issue, researchers have developed alternative methods, such as integral identification 17 and weak form identification 18. Deep learning architectures offer highly expressive models for function approximation and have demonstrated success in various scenarios 19-21. Nonetheless, these models often exhibit a bias towards certain dynamic representations that do not strictly enforce the system's symmetries and conservation laws. Consequently, their generalization ability suffers, leading to physically implausible outcomes when applied to new, unseen states. This characteristic also contributes to their high variance and challenges in interpretation 22. Moreover, training these models typically necessitates substantial data sets and lengthy computation times, rendering them impractical for many real-world applications. Thus, it is imperative to develop data-driven methods that yield physically plausible results 23-26.
Physics-guided machine learning methods have emerged as a hybrid method that integrates the principles of physics with deep learning architectures. This novel method incorporates the physical constraints and geometric properties of the underlying system during the design and learning processes of neural networks. By leveraging the function approximation capabilities of neural networks, existing knowledge of the system's physics can be incorporated into the construction of physically constrained neural networks, leading to improved design, efficiency, and generalization capabilities 27. In recent years, physically informed neural networks (PINNs) have been introduced as a means to incorporate the underlying physics during the training process and enhance desired system properties, addressing the limitations of classical neural networks 28. For instance, Chen et al. developed neural ordinary differential equations (Node) to uncover hidden ordinary differential equations from discrete data 29. However, these models often fail to preserve system energy and exhibit poor generalization capabilities.
To address these limitations, researchers have explored integrating physical principles, such as differential equations and symmetries, into deep neural networks as constraints 30,31. Lutter et al. proposed deep Lagrangian networks (DeLaNs), which employ two deep networks to model rigid body dynamics by parameterizing kinetic and potential energies 12. This formulation ensures the preservation of system energy, leading to superior long-term prediction and control performance. Greydanus et al. introduced Hamiltonian neural networks (HNN) 32, followed by Cranmer et al. who proposed Lagrangian neural networks (LNN) 33. Both approaches model the system holistically, regressing central quantities such as Lagrangian or Hamiltonian quantities to represent the system's dynamics and maintain physical properties like energy conservation and symmetry. To explicitly enforce constraints and simplify the learning problem, Finzi et al. presented constrained Hamiltonian neural networks (CHNN) and constrained Lagrangian neural networks (CLNN) 34. These frameworks embed the system in a Cartesian coordinate system and leverage Lagrange multipliers to explicitly enforce the constraints. As a result, they achieve simplification of the learning problem. Lu et al. introduced modular Lagrangian networks (ModLaNet), which employ Euler Lagrangian equations to independently model system elements 35. This approach demonstrates improved performance in multibody tasks. Moreover, Gruver et al. proposed mechanics neural networks (MechanicsNN), utilizing an induction bias to deconstruct the HNN model and enhance computational efficiency 36. These models aim to strike a balance between accuracy and computational cost. However, these methods do not fully consider the impact of rigid conditions on accuracy, necessitating further research.
Through an in-depth exploration of the intrinsic dynamic properties inherent in multibody systems, a comprehensive understanding of their behavior in both scientific research and engineering applications can be achieved. This, in turn, equips us with indispensable tools for predicting and managing system behavior, thereby facilitating the resolution of critical problems. The dynamic modeling and prediction of multiphysics and multiscale systems present ongoing scientific challenges, as underscored by Karniadakis et al. 37. It is imperative to acknowledge that these systems often manifest rigid characteristics that exert a significant impact on the learning process. This effect becomes particularly pronounced in scenarios where parameters of system components undergo substantial changes or in the convergence of slow variables characterized by extensive ranges of motion and fast variables involving elastic deformations. As a result, neural network methods face limitations in addressing the inherent complexities of specific multibody dynamics equations. Given these constraints and drawing upon existing research findings, we posit a novel approach grounded in multiscale differential-algebraic neural networks (MDANN). Our method employs a system of differential-algebraic equations (DAE) with constraints explicitly enforced using Lagrange multipliers. This approach facilitates the learning of Lagrangian quantities, contributing to a more nuanced understanding of the system dynamics. Furthermore, the effectiveness of our proposed method is demonstrated through its application in a dynamically controlled environment. We introduce a multiscale module that integrates information from different frequencies, thereby enabling the learning of subprocesses with notable speed variations within the system. We conducted experimental validation of our proposed method, investigating coupled pendulum systems and double pendulum systems. The outcomes of these experiments provide compelling evidence substantiating the efficacy of our approach in acquiring a profound understanding of dynamic systems. Additionally, we extend the application of our method to scalable structures, leveraging it to enhance precision in determining control forces. The results obtained from these applications distinctly showcase the robustness of our approach, affirming its capability to refine control strategies for dynamic systems.
The rest of the paper is organized as follows. Section 2 provides essential background knowledge to facilitate a better understanding of the subsequent discussions. Section 3 presents the MDANN method along with the technical implementation details.. Section 4 showcases a series of numerical experiments. The paper concludes in Section 5.

2    |  Learning dynamics
The standard frameworks for Hamiltonian and Lagrangian dynamics are typically presented as sets of first- and second-order ODE, respectively. In 2018, Chen et al.29 proposed Node method, a continuous DNN-based approach that utilizes neural networks to represent the time-continuous dynamics of hidden states.
The Node method models the time evolution of a hidden unit as a constant neural differential equation, as shown in Equation (1):
		(1)
where t represents time, x(t) is a continuous representation of the hidden state, and θ refers to the network weights and biases. The evolution function f(x(t),t,θ), parameterized by the neural network, describes the dynamics of the hidden state over time. By solving the initial value problem (IVP) using the initial conditions x(t0), the hidden state x(t1) at the next moment is obtained. Therefore, the time evolution of the state x(t) can be represented as a time integral, as shown in Equation (2):
		(2)
where ODESolve is an ODE numerical solver. To improve memory utilization efficiency, the concomitant sensitivity method is used during backpropagation 29. Furthermore, the Node method allows for the incorporation of prior knowledge of physical principles into the network, enabling the parameterization of unknown physical quantities in the dynamics, such as mass, potential energy, Lagrangian, and Hamiltonian quantities.

3    |  Proposed method
This section introduces our proposed MDANN method. The method comprises two key modules: the Lagrangian mechanics module and the multiscale module. The Lagrangian mechanics module facilitates the learning of Lagrangian quantities by employing a system of DAE with explicit constraints. On the other hand, the multiscale module utilizes radial scaling to transform high-frequency components into low-frequency components, thereby enabling the learning of physical processes at various frequencies within Lagrangian quantities.
3.1  |Lagrangian Mechanics Module
The utilization of Lagrangian quantities to ensure energy conservation in a system has proven invaluable for comprehensively understanding the underlying physical processes 7. By explicitly incorporating constraints, the learning of Lagrangian quantities is further enhanced, leading to improved data efficiency and prediction accuracy.
In this module, we employ the Lagrange multiplier method, which utilizes a system of DAE to explicitly handle constraints and obtain the Lagrangian quantities of the system. The module consists of two key components: a differential component and an algebraic component. The differential component focuses on the ODE of motion, which describe the temporal evolution of an object or system. It allows for the conservation of the Lagrangian quantity of the system. By considering variables such as position, velocity, and acceleration, we can infer the dynamic behavior over time. Differential equations provide the fundamental framework for describing the motion of an object or system. On the other hand, the algebraic component involves the algebraic constraint equations that capture the relationships within the system, including binding forces and other constraints. These equations effectively represent the interactions and constraints among the different components. Figure 1 depicts a schematic diagram of the Lagrangian mechanics module.


Figure 1 Schematic diagram of Lagrangian mechanics module.
The system is characterized by a set of Cartesian coordinates q(t) that represent the configuration of a rigid body at time t. The Lagrangian function of the system, defined in Equation (3), is a fundamental component of our proposed methodology:
		(3)
where T represents the kinetic energy of the system, and V represents its potential energy. To efficiently learn Lagrangian quantities of physical systems, we parameterize the kinetic and potential energy using two separate neural networks. We leverage Lagrangian quantities as prior knowledge in this process.
In the context of mechanical systems, the kinetic energy T of the system is determined by Equation (4):
		(4)
where the generalized mass matrix  which is a constant matrix in Cartesian coordinates and does not depend on the state q(t). Therefore, learning the constant values of this matrix using a neural network can simplify the form of the Lagrangian function. The dynamics of a multi-body system can be expressed using the DAE as shown in Equation (5):
		(5)
The above equation describes the system using the generalized velocity , the Lagrange multiplier λ, the constraint equation  for the position coordinate array q, the generalized force vector F, and the Jacobi matrix of the constraint equation, . To obtain the velocity constraint equation  and the acceleration constraint equation  for the system, we need to solve the constraint equation  for the first and second-order derivatives with respect to time t, respectively. The velocity constraint equation, as shown in Equation (6), is given by:
		(6)
Similarly, the acceleration constraint equation, as shown in Equation (7), is given by:
		(7)
These equations result in the system of index-1 DAE, given by Equation (8):
		(8)
where the generalized force array . Equation (8) can be reformulated as Equation (9), which explicitly expresses the acceleration of the generalized coordinates q(t) and the Lagrange multipliers λ as a function of the other variables. This reformulation can be particularly helpful in numerical integration of the DAE.
		(9)
3.2  |Multiscale Module
Dynamical systems often exhibit multifrequency phenomena, particularly when their components vary significantly in parameters or involve a combination of slow variables with a wide range of motion and fast variables with elastic deformation. Consequently, the solutions of such systems comprise multiple frequency components that are superimposed on each other. DNNs excel at processing data with low-frequency content, as supported by the frequency principle (F-principle) 38. DNNs can rapidly learn the low-frequency content of data and achieve commendable generalization accuracy. However, neural networks often struggle when confronted with high-frequency data, leading to reduced convergence or even non-convergence of the learning method. In the domain of multi-body dynamics, the learning of dynamical systems poses challenges due to the frequency disparities between the motions of objects within the system.
To tackle this challenge, we employ a multiscale structure to preprocess the input data and extract frequency features that are better suited for learning. Specifically, we adopt the multiscale structure proposed by Liu. 39. This approach has demonstrated its efficacy in facilitating the rapid learning of high-frequency components and expediting the solution of partial differential equations in comparison to traditional fully connected network structures. The multiscale module employs radial scaling to convert solution content from higher frequency ranges to lower ones, thereby rendering the solution content easier to learn. The module takes the state variable q as input and produces the potential energy V of the system as output. A schematic diagram representation of the module is depicted in Figure 2.


Figure 2 Schematic diagram of multiscale module.
In our multiscale module, we utilize the linear combination property of energy to establish the potential energy of the system 35. The potential energy comprises two main components: the potential energy, denoted as Vi, between the elements and the environment, and the potential energy, denoted as Vij, between the elements themselves. The expression for the potential energy is presented in Equation (10).
		(10)
where the weight parameters of Vi and Vij are denoted by ci and cij, respectively. Additionally, Vij is symmetric, that is, Vij=Vji.
To obtain consistent frequency accuracy solutions, the position coordinates q are segmented into different frequency ranges using radial scaling. This segmentation allows for the utilization of m parallel subneural networks, denoted as Vθ, which are responsible for calculating the potential energy within each frequency range. Finally, we calculate the potential energy using a weighted summation, as shown in Equation (11).
		(11)
where αk and θk denote the scaling factor and network parameters of the k-th sub-network, respectively. Additionally, we use residual connections to address the gradient vanishing issue.
During the training process, our model takes the system's states  as inputs and predicts the corresponding states . To update the network parameters, we utilize the L2 loss function, which is expressed as follows in Equation (12):
		(12)
where  and  represent the true and predicted system states, respectively. N represents the number of samples in the dataset. This loss function quantifies the discrepancy between the predicted and actual states, guiding the adjustment of network parameters during the training process.