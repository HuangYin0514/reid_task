The objective of dynamical system learning tasks is to forecast the future behavior of a system by leveraging observed data 1. Dynamical systems provide a mathematical framework to describe the evolution of natural phenomena across time and space, commonly represented using differential equations. Solving these equations allows predictions about the future state of dynamical systems. Understanding complex physical dynamics across various spatial and temporal scales poses a significant research challenge, garnering attention from numerous scholars and having practical implications 2-6.
Prediction in dynamical systems primarily relies on creating models derived from first principles. However, due to incomplete knowledge, these models based on physical laws often oversimplify or misrepresent the underlying structure of dynamical systems 7. As a result, they tend to exhibit high biases and modeling errors that cannot be rectified by optimizing a few parameters. Consequently, it becomes essential to employ dynamical system learning methods that can accurately identify valid dynamic models based on observed trajectories. These methods play a crucial role in the analysis, simulation, and control of dynamic systems 8-10.
Dynamical systems learning methods can be broadly classified into two categories: purely data-driven methods and physics-guided machine learning methods 11. The availability of experimental data has led to the increasing popularity of purely data-driven methods, which utilize forward or backward models to fit the training data without requiring extensive involvement in model design or the derivation of learning and inference processes 12-15. Among purely data-driven methods, neural networks have emerged as the dominant technology due to their exceptional ability to efficiently capture high-dimensional spatio-temporal dynamics from large datasets. One notable method in this category is the sparse identification of nonlinear dynamics (SINDy), which represents dynamic nonlinear differential equations as linear combinations of nonlinear candidate functions and approximates the model through sparse regression 16. However, SINDy's reliance on numerical differentiation renders it sensitive to data noise, particularly when higher-order derivatives are involved. To mitigate this issue, researchers have developed alternative methods, such as integral identification 17 and weak form identification 18. Deep learning architectures offer highly expressive models for function approximation and have demonstrated success in various scenarios 19-21. Nonetheless, these models often exhibit a bias towards certain dynamic representations that do not strictly enforce the system's symmetries and conservation laws. Consequently, their generalization ability suffers, leading to physically implausible outcomes when applied to new, unseen states. This characteristic also contributes to their high variance and challenges in interpretation 22. Moreover, training these models typically necessitates substantial datasets and lengthy computation times, rendering them impractical for many real-world applications. Thus, it is imperative to develop data-driven methods that yield physically plausible results 23-26.
Physics-guided machine learning methods have emerged as a hybrid method that integrates the principles of physics with deep learning architectures. This novel method incorporates the physical constraints and geometric properties of the underlying system during the design and learning processes of neural networks. By leveraging the function approximation capabilities of neural networks, existing knowledge of the system's physics can be incorporated into the construction of physically constrained neural networks, leading to improved design, efficiency, and generalization capabilities 27. In recent years, physically-informed neural networks (PINNs) have been introduced as a means to incorporate the underlying physics during the training process and enhance desired system properties, addressing the limitations of classical neural networks 28. For instance, Chen et al. developed neural ordinary differential equations (Node) to uncover hidden ordinary differential equations from discrete data 29. However, these models often fail to preserve system energy and exhibit poor generalization capabilities.
To address these limitations, researchers have explored integrating physical principles, such as differential equations and symmetries, into deep neural networks as constraints 30,31. Lutter et al. proposed deep Lagrangian networks (DeLaNs), which employ two deep networks to model rigid body dynamics by parameterizing kinetic and potential energies 12. This formulation ensures the preservation of system energy, leading to superior long-term prediction and control performance. Greydanus et al. introduced Hamiltonian neural networks (HNN) 32, followed by Cranmer et al. who proposed Lagrangian neural networks (LNN) 33. Both approaches model the system holistically, regressing central quantities such as Lagrangian or Hamiltonian quantities to represent the system's dynamics and maintain physical properties like energy conservation and symmetry. To explicitly enforce constraints and simplify the learning problem, Finzi et al. presented constrained Hamiltonian neural networks (CHNN) and constrained Lagrangian neural networks (CLNN) 34. These frameworks embed the system in a Cartesian coordinate system and leverage Lagrange multipliers to explicitly enforce the constraints. As a result, they achieve simplification of the learning problem. Lu et al. introduced modular Lagrangian networks (ModLaNet), which employ Euler Lagrangian equations to independently model system elements 35. This approach demonstrates improved performance in multi-body tasks. Moreover, Gruver et al. proposed mechanics neural networks (MechanicsNN), utilizing an induction bias to deconstruct the HNN model and enhance computational efficiency 36. These models aim to strike a balance between accuracy and computational cost. However, these methods do not fully consider the impact of rigid conditions on accuracy, necessitating further research.
Through an in-depth exploration of the intrinsic dynamic properties inherent in multi-body systems, a comprehensive understanding of their behavior in both scientific research and engineering applications can be achieved. This, in turn, equips us with indispensable tools for predicting and managing system behavior, thereby facilitating the resolution of critical problems. The dynamic modeling and prediction of multiphysics and multiscale systems present ongoing scientific challenges, as underscored by Karniadakis et al. 37. It is imperative to acknowledge that these systems often manifest rigid characteristics that exert a significant impact on the learning process. This effect becomes particularly pronounced in scenarios where parameters of system components undergo substantial changes or in the convergence of slow variables characterized by extensive ranges of motion and fast variables involving elastic deformations. As a result, neural network methods face limitations in addressing the inherent complexities of specific multibody dynamics equations. Given these constraints and drawing upon existing research findings, we posit a novel approach grounded in multiscale differential-algebraic neural networks (MDANN). Our method employs a system of DAE with constraints explicitly enforced using Lagrange multipliers. This approach facilitates the learning of Lagrangian quantities, contributing to a more nuanced understanding of the system dynamics. Furthermore, the effectiveness of our proposed method is demonstrated through its application in a dynamically controlled environment. We introduce a multiscale module that integrates information from different frequencies, thereby enabling the learning of subprocesses with notable speed variations within the system. We conducted experimental validation of our proposed method, investigating coupled pendulum systems and double pendulum systems. The outcomes of these experiments provide compelling evidence substantiating the efficacy of our approach in acquiring a profound understanding of dynamic systems. Additionally, we extend the application of our method to scalable structures, leveraging it to enhance precision in determining control forces. The results obtained from these applications distinctly showcase the robustness of our approach, affirming its capability to refine control strategies for dynamic systems.
The rest of the paper is organized as follows. Section 2 provides essential background knowledge to facilitate a better understanding of the subsequent discussions. In Section 3, the  MDANN method is presented along with the technical implementation details. Section 4 showcases a series of numerical experiments. The paper concludes in Section 5.