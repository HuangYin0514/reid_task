2024-03-08 14:22:43 INFO ##################################################
2024-03-08 14:22:43 INFO Task: PCB
2024-03-08 14:22:43 INFO Using device: cuda
2024-03-08 14:22:43 INFO Using data type: torch.float32
2024-03-08 14:22:43 INFO Number of GPUs: 1
2024-03-08 14:22:43 INFO GPU is_available: True
2024-03-08 14:22:43 INFO GPU name: NVIDIA GeForce RTX 3060
2024-03-08 14:22:43 INFO CUDA version: 12.1
2024-03-08 14:22:43 INFO Current device id: 0
2024-03-08 14:22:43 INFO ##################################################
2024-03-08 14:23:48 INFO Epoch 1/60	Training Loss: 35.1256	Time remaining is 1h:4m
2024-03-08 14:24:50 INFO Epoch 2/60	Training Loss: 30.5131	Time remaining is 1h:2m
2024-03-08 14:25:53 INFO Epoch 3/60	Training Loss: 26.2884	Time remaining is 1h:1m
2024-03-08 14:26:56 INFO Epoch 4/60	Training Loss: 22.6936	Time remaining is 0h:60m
2024-03-08 14:27:58 INFO Epoch 5/60	Training Loss: 19.8356	Time remaining is 0h:59m
2024-03-08 14:29:00 INFO Epoch 6/60	Training Loss: 17.5087	Time remaining is 0h:57m
2024-03-08 14:30:03 INFO Epoch 7/60	Training Loss: 15.6941	Time remaining is 0h:56m
2024-03-08 14:31:06 INFO Epoch 8/60	Training Loss: 14.2470	Time remaining is 0h:55m
2024-03-08 14:32:08 INFO Epoch 9/60	Training Loss: 12.9497	Time remaining is 0h:54m
2024-03-08 14:33:10 INFO Epoch 10/60	Training Loss: 11.9969	Time remaining is 0h:53m
2024-03-08 14:34:12 INFO Epoch 11/60	Training Loss: 11.1229	Time remaining is 0h:52m
2024-03-08 14:35:15 INFO Epoch 12/60	Training Loss: 10.5458	Time remaining is 0h:51m
2024-03-08 14:36:17 INFO Epoch 13/60	Training Loss: 9.9521	Time remaining is 0h:50m
2024-03-08 14:37:20 INFO Epoch 14/60	Training Loss: 9.4965	Time remaining is 0h:49m
2024-03-08 14:38:22 INFO Epoch 15/60	Training Loss: 9.0521	Time remaining is 0h:48m
2024-03-08 14:39:25 INFO Epoch 16/60	Training Loss: 8.6878	Time remaining is 0h:47m
2024-03-08 14:40:27 INFO Epoch 17/60	Training Loss: 8.5333	Time remaining is 0h:46m
2024-03-08 14:41:30 INFO Epoch 18/60	Training Loss: 8.1168	Time remaining is 0h:45m
2024-03-08 14:42:32 INFO Epoch 19/60	Training Loss: 7.9385	Time remaining is 0h:44m
2024-03-08 14:43:35 INFO Epoch 20/60	Training Loss: 7.8128	Time remaining is 0h:43m
2024-03-08 14:44:06 INFO Testing: dataset_name: market1501 top1:0.9041 top5:0.9605 top10:0.9754 mAP:0.7382
2024-03-08 14:45:08 INFO Epoch 21/60	Training Loss: 7.0298	Time remaining is 0h:43m
2024-03-08 14:46:11 INFO Epoch 22/60	Training Loss: 6.4779	Time remaining is 0h:42m
2024-03-08 14:47:13 INFO Epoch 23/60	Training Loss: 6.3768	Time remaining is 0h:40m
2024-03-08 14:48:15 INFO Epoch 24/60	Training Loss: 6.3791	Time remaining is 0h:39m
2024-03-08 14:49:18 INFO Epoch 25/60	Training Loss: 6.3023	Time remaining is 0h:38m
2024-03-08 14:50:20 INFO Epoch 26/60	Training Loss: 6.2869	Time remaining is 0h:37m
2024-03-08 14:51:23 INFO Epoch 27/60	Training Loss: 6.2507	Time remaining is 0h:36m
2024-03-08 14:52:25 INFO Epoch 28/60	Training Loss: 6.2012	Time remaining is 0h:35m
2024-03-08 14:53:27 INFO Epoch 29/60	Training Loss: 6.2141	Time remaining is 0h:34m
2024-03-08 14:54:30 INFO Epoch 30/60	Training Loss: 6.1946	Time remaining is 0h:33m
2024-03-08 14:55:32 INFO Epoch 31/60	Training Loss: 6.1547	Time remaining is 0h:32m
2024-03-08 14:56:35 INFO Epoch 32/60	Training Loss: 6.1745	Time remaining is 0h:31m
2024-03-08 14:57:37 INFO Epoch 33/60	Training Loss: 6.1568	Time remaining is 0h:30m
2024-03-08 14:58:40 INFO Epoch 34/60	Training Loss: 6.1469	Time remaining is 0h:29m
2024-03-08 14:59:42 INFO Epoch 35/60	Training Loss: 6.1019	Time remaining is 0h:27m
2024-03-08 15:00:45 INFO Epoch 36/60	Training Loss: 6.1273	Time remaining is 0h:26m
2024-03-08 15:01:47 INFO Epoch 37/60	Training Loss: 6.1114	Time remaining is 0h:25m
2024-03-08 15:02:50 INFO Epoch 38/60	Training Loss: 6.1073	Time remaining is 0h:24m
2024-03-08 15:03:53 INFO Epoch 39/60	Training Loss: 6.1016	Time remaining is 0h:23m
2024-03-08 15:04:55 INFO Epoch 40/60	Training Loss: 6.0530	Time remaining is 0h:22m
2024-03-08 15:05:26 INFO Testing: dataset_name: market1501 top1:0.9296 top5:0.9700 top10:0.9819 mAP:0.7905
2024-03-08 15:06:29 INFO Epoch 41/60	Training Loss: 6.0748	Time remaining is 0h:21m
2024-03-08 15:07:31 INFO Epoch 42/60	Training Loss: 6.0599	Time remaining is 0h:20m
2024-03-08 15:08:34 INFO Epoch 43/60	Training Loss: 6.0368	Time remaining is 0h:19m
2024-03-08 15:09:36 INFO Epoch 44/60	Training Loss: 6.0659	Time remaining is 0h:18m
2024-03-08 15:10:39 INFO Epoch 45/60	Training Loss: 6.0555	Time remaining is 0h:17m
2024-03-08 15:11:42 INFO Epoch 46/60	Training Loss: 6.0960	Time remaining is 0h:16m
2024-03-08 15:12:44 INFO Epoch 47/60	Training Loss: 6.0234	Time remaining is 0h:15m
2024-03-08 15:13:46 INFO Epoch 48/60	Training Loss: 6.0608	Time remaining is 0h:14m
2024-03-08 15:14:49 INFO Epoch 49/60	Training Loss: 6.0217	Time remaining is 0h:13m
2024-03-08 15:15:51 INFO Epoch 50/60	Training Loss: 6.0220	Time remaining is 0h:12m
2024-03-08 15:16:54 INFO Epoch 51/60	Training Loss: 6.0872	Time remaining is 0h:11m
2024-03-08 15:17:57 INFO Epoch 52/60	Training Loss: 6.0615	Time remaining is 0h:10m
2024-03-08 15:19:00 INFO Epoch 53/60	Training Loss: 6.0615	Time remaining is 0h:8m
2024-03-08 15:20:02 INFO Epoch 54/60	Training Loss: 6.0217	Time remaining is 0h:7m
2024-03-08 15:21:05 INFO Epoch 55/60	Training Loss: 6.0509	Time remaining is 0h:6m
2024-03-08 15:22:07 INFO Epoch 56/60	Training Loss: 6.0514	Time remaining is 0h:5m
2024-03-08 15:23:10 INFO Epoch 57/60	Training Loss: 6.0569	Time remaining is 0h:4m
2024-03-08 15:24:13 INFO Epoch 58/60	Training Loss: 6.0566	Time remaining is 0h:3m
2024-03-08 15:25:16 INFO Epoch 59/60	Training Loss: 6.0495	Time remaining is 0h:2m
2024-03-08 15:26:18 INFO Epoch 60/60	Training Loss: 6.0529	Time remaining is 0h:1m
2024-03-08 15:26:50 INFO Testing: dataset_name: market1501 top1:0.9305 top5:0.9709 top10:0.9813 mAP:0.7927
2024-03-08 15:26:50 INFO The running time of training: 3.84622e+03 s
2024-03-08 15:27:12 INFO ##################################################
2024-03-08 15:27:12 INFO Task: PCB
2024-03-08 15:27:12 INFO Using device: cuda
2024-03-08 15:27:12 INFO Using data type: torch.float32
